{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import wave\n",
    "import pyaudio\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import tensorflow.keras.backend as K\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 録音部分\n",
    "---\n",
    "何秒に一度サンプリングするか？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rec_audio(filename):\n",
    "    rec_time = 3            # 録音時間[s]\n",
    "\n",
    "    file_path = fiename \n",
    "    fmt = pyaudio.paInt16  # 音声のフォーマット\n",
    "    ch = 1              # チャンネル1(モノラル)\n",
    "    sampling_rate = 44100 # サンプリング周波数\n",
    "    chunk = 2**11       # チャンク（データ点数）\n",
    "    audio = pyaudio.PyAudio()\n",
    "    index = 1 # 録音デバイスのインデックス番号（デフォルト1）\n",
    "\n",
    "    stream = audio.open(format=fmt, channels=ch, rate=sampling_rate, input=True,\n",
    "                        input_device_index = index,\n",
    "                        frames_per_buffer=chunk)\n",
    "    # 録音処理\n",
    "    frames = []\n",
    "    for i in range(0, int(sampling_rate / chunk * rec_time)):\n",
    "        data = stream.read(chunk)\n",
    "        frames.append(data)\n",
    "\n",
    "    # 録音終了処理\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    audio.terminate()\n",
    "\n",
    "    # 録音データをファイルに保存\n",
    "    wav = wave.open(file_path, 'wb')\n",
    "    wav.setnchannels(ch)\n",
    "    wav.setsampwidth(audio.get_sample_size(fmt))\n",
    "    wav.setframerate(sampling_rate)\n",
    "    wav.writeframes(b''.join(frames))\n",
    "    wav.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wave_load(filename):\n",
    "    # open wave file\n",
    "    wf = wave.open(filename,'r')\n",
    "    channels = wf.getnchannels()\n",
    "\n",
    "    # load wave data\n",
    "    chunk_size = wf.getnframes()\n",
    "    amp  = (2**8) ** wf.getsampwidth() / 2\n",
    "    data = wf.readframes(chunk_size)   # バイナリ読み込み\n",
    "    data = np.frombuffer(data,'int16') # intに変換\n",
    "    data = data / amp                  # 振幅正規化\n",
    "    data = data[::channels]\n",
    "   \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fft_load(size,filename):\n",
    "    '''\n",
    "    st = サンプリング開始位置 [s]\n",
    "    size = FFTのサンプル数（２＊＊ｎ）\n",
    "    filename = オーディオファイルのディレクトリ\n",
    "\n",
    "    '''\n",
    "    #st=44100*st\n",
    "    \n",
    "    \n",
    "    hammingWindow = np.hamming(size)    # ハミング窓\n",
    "    fs = 44100 #サンプリングレート\n",
    "    d = 1.0 / fs #サンプリングレートの逆数\n",
    "    freqList = np.fft.fftfreq(size, d)\n",
    "    \n",
    "    rcParams[\"figure.figsize\"]=16,8\n",
    "    \n",
    "    wave = wave_load(filename)\n",
    "    #print(len(wave))\n",
    "    \n",
    "    #10000Hzごとにサンプルを10点取ってそれを平均したものを学習用の音声スペクトルデータとして用いる。\n",
    "    for i in range(10):\n",
    "        st=i*10000\n",
    "        windowedData = hammingWindow * wave[0:size]  # 切り出した波形データ（窓関数あり）\n",
    "        sample_data = np.fft.fft(windowedData)\n",
    "        sample_data = sample_data / max(abs(sample_data)) # 0~1正規化\n",
    "        if(i==0):\n",
    "            data = sample_data\n",
    "        else:\n",
    "            data = (i*data+sample_data)/(i+1)\n",
    "    \n",
    "    data=abs(data)/max(abs(data))\n",
    "    #print(type(data[0]))\n",
    "    \n",
    "    plt.plot(freqList,abs(data))\n",
    "         \n",
    "    plt.axis([0,fs/8,0,1]) #第二引数でグラフのy軸方向の範囲指定\n",
    "    plt.title(\"audio_FFT_data\")\n",
    "    plt.xlabel(\"Frequency[Hz]\")\n",
    "    plt.ylabel(\"amplitude spectrum\")\n",
    "    #plt.show()\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=keras.models.load_model(\"./model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename=\"./t_fal_audio/sound_data.wav\"\n",
    "size=2**12\n",
    "\n",
    "interval=2\n",
    "\n",
    "temp_set=np.array([])\n",
    "\n",
    "i=0\n",
    "while(300-i*(3+interval)):\n",
    "    \n",
    "    sleep(interval)\n",
    "    \n",
    "    rec_audio(filename)\n",
    "    fft_load(size, filename)\n",
    "    temp=model.predict(data)\n",
    "    \n",
    "    temp_set=np.append(temp_set, temp)\n",
    "    \n",
    "    i+=1\n",
    "    \n",
    "pritn(temp_set)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
